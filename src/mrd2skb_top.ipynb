{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRD2SKB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Preliminary setup.\n",
    "\n",
    "# ## Library imports.\n",
    "\n",
    "# Standard libraries.\n",
    "import contextlib\n",
    "import copy\n",
    "import functools\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import inspect\n",
    "import operator\n",
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# External libraries.\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd  # check faster alternatives?\n",
    "# https://www.datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray\n",
    "\n",
    "# ML specific libraries.\n",
    "import gensim\n",
    "import spacy\n",
    "import sklearn.feature_extraction.text as skl_feat_text\n",
    "\n",
    "# Following are some libraries for fast gpu computations.\n",
    "# import dask.bag as db\n",
    "# import dask.array as da\n",
    "# import dask.dataframe as dd\n",
    "# import cupyx as cpx\n",
    "# import cupy as cp\n",
    "# import numba\n",
    "# import swifter\n",
    "\n",
    "# Jupyter notebook libraries.\n",
    "from IPython.display import display  # enable this when converting to a script.\n",
    "from IPython import get_ipython\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "# ## General path configurations.\n",
    "\n",
    "# Get script file absolute path.\n",
    "file_abspath = None\n",
    "if '__file__' not in globals():\n",
    "    # We are in a .ipynb notebook, and presumably running in vscode.\n",
    "    ip = get_ipython()\n",
    "    file_abspath = ip.user_ns.get('__vsc_ipynb_file__', None)\n",
    "else:\n",
    "    # We are in a .py script.\n",
    "    file_abspath = os.path.abspath(__file__)\n",
    "\n",
    "file_basename = os.path.basename(file_abspath)\n",
    "file_dirname = os.path.dirname(file_abspath)\n",
    "file_name, file_ext = os.path.splitext(file_basename)\n",
    "\n",
    "\n",
    "# ## Preconfigurations for module logger.\n",
    "# Get the root logger, this is sometimes useful.\n",
    "logger_root = logging.getLogger()\n",
    "# Create logger with current module.\n",
    "if 'logger' not in vars():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    log_formatter = logging.Formatter(\"[%(levelname)s]: %(message)s\")\n",
    "    log_formatter_verbose = logging.Formatter(\n",
    "        \"[%(levelname)s]: %(asctime)s; File %(filename)s, line %(lineno)d, in %(funcName)s: %(message)s\")\n",
    "    # Create console log handler.\n",
    "    log_ch = logging.StreamHandler()\n",
    "    log_ch.setFormatter(log_formatter)\n",
    "    logger.addHandler(log_ch)\n",
    "    # Create file log handler.\n",
    "    log_fh = logging.FileHandler(f\"{file_abspath}.log\")\n",
    "    log_fh.setFormatter(log_formatter)\n",
    "    # logger.addHandler(log_fh)\n",
    "\n",
    "# Set log handler levels.\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# log_ch.setLevel(logging.INFO)\n",
    "# log_fh.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def log_obj(level, obj):\n",
    "    \"\"\"Log an object, using ipython display function.\"\"\"\n",
    "    # Obtain the name of the object.\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    obj_name = [obj_name for obj_name, obj_val in callers_local_vars\n",
    "                if obj_val is obj][0]\n",
    "    # log and display the object if log level permits.\n",
    "    if level >= logger.level:\n",
    "        logger.log(level, f\"{obj_name} = \")\n",
    "        display(obj)\n",
    "\n",
    "# ## GPU configurations.\n",
    "\n",
    "# Check GPU availability for supported libraries.\n",
    "# logger.debug(f\"{torch.cuda.is_available() = }\")\n",
    "# logger.debug(f\"{torch.cuda.get_device_name(0) = }\")\n",
    "# logger.debug(f\"{tf.config.list_physical_devices('GPU') = }\")\n",
    "\n",
    "# Check BLAS and LAPACK availability for supported libraries.\n",
    "# logger.debug(f\"{np.show_config() = }\")\n",
    "\n",
    "\n",
    "# Use GPU if applicable.\n",
    "logger.debug(f\"{spacy.prefer_gpu() = }\")\n",
    "\n",
    "\n",
    "# ## Timer configurations.\n",
    "\n",
    "\n",
    "def timer_dec(func):\n",
    "    \"\"\"Print the runtime of the decorated function.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        \"\"\"Wrap the function and time it.\"\"\"\n",
    "        func_name = repr(func.__name__)\n",
    "        logger.debug(f\"{func_name} Start\")\n",
    "        start_time = time.perf_counter()\n",
    "        value = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        run_time = end_time - start_time\n",
    "        logger.debug(f\"{func_name} End\")\n",
    "        logger.info(f\"{func_name} runtime: {run_time:.6f} secs.\")\n",
    "        return value\n",
    "    return wrapper_timer\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timer_con():\n",
    "    \"\"\"Print the runtime of code block in the managed context.\"\"\"\n",
    "    logger.debug(f\"Timer Start\")\n",
    "    start_time = time.perf_counter()\n",
    "    yield\n",
    "    end_time = time.perf_counter()\n",
    "    run_time = end_time - start_time\n",
    "    logger.debug(f\"Timer End\")\n",
    "    logger.info(f\"Runtime: {run_time:.6f} secs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Project specific configurations.\n",
    "\n",
    "\"\"\" MRD2SKB Project Directory Structure\n",
    "mrd2skb/\n",
    "    data/\n",
    "        input/\n",
    "            dict_compact_wordnet.csv\n",
    "        interm/\n",
    "            preproc_dict_df.pkl\n",
    "            wsd_dict_df.pkl\n",
    "        output/\n",
    "            mrd2skb_mtx_df.pkl\n",
    "            mrd2skb_list_df.pkl\n",
    "            mrd2skb_sememes.txt\n",
    "            mrd2skb_skb.txt\n",
    "            mrd2skb_dict.npy\n",
    "            mrd2skb_sememes.npy\n",
    "            mrd2skb_valid_words.npy\n",
    "            models/\n",
    "                ...\n",
    "        backup/\n",
    "            mrd2skb_kro_bkp_x0=xx_x1=xx_x2=xx/\n",
    "                interm/\n",
    "                    ...\n",
    "                output/\n",
    "                    ...\n",
    "            mrd2skb_top_bkp_x0=xx_x1=xx_x2=xx/\n",
    "                interm/\n",
    "                    ...\n",
    "                output/\n",
    "                    ...\n",
    "    src/\n",
    "        mrd2skb_kro.ipynb\n",
    "        mrd2skb_top.ipynb\n",
    "\"\"\"\n",
    "\n",
    "os.chdir(file_dirname)\n",
    "\n",
    "# ## Project specific paths and directories.\n",
    "# Relative paths of input output directories.\n",
    "prj_root_dir = \"../\"\n",
    "prj_data_dir = prj_root_dir + \"data/\"\n",
    "input_data_dir = prj_data_dir + \"input/\"\n",
    "interm_data_dir = prj_data_dir + \"interm/\"\n",
    "output_data_dir = prj_data_dir + \"output/\"\n",
    "backup_data_dir = prj_data_dir + \"backup/\"\n",
    "\n",
    "# Create directories if they do not exist.\n",
    "pathlib.Path(interm_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(output_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(backup_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ## Project specific variables.\n",
    "# Column names for terms and definitions from dictionary dataset.\n",
    "term_colname = \"lemma\"\n",
    "pos_colname = \"posname\"\n",
    "senseid_colname = \"senseid\"\n",
    "defn_colname = \"definition\"\n",
    "\n",
    "# PoS tag codes dictionary. depends on the MRD.\n",
    "# For WordNet, see ss_type under https://wordnet.princeton.edu/documentation/wndb5wn.\n",
    "# The following code may be useful for determining these values:\n",
    "# preproc_dict_df[pos_colname].unique()\n",
    "pos_code_dict = {\n",
    "    \"noun\": \"n\",\n",
    "    \"verb\": \"v\",\n",
    "    \"adjective\": \"a\",\n",
    "    \"adjective satellite\": \"a\",  # this is actually \"s\", but we prefer \"a\".\n",
    "    \"adverb\": \"r\",\n",
    "    \"default\": \"x\",\n",
    "}\n",
    "\n",
    "# The following is useful for generating the output skb files.\n",
    "pos_code_inv_dict = {\n",
    "    \"n\": \"noun\",\n",
    "    \"v\": \"verb\",\n",
    "    \"a\": \"adjective\",\n",
    "    \"r\": \"adverb\",\n",
    "    \"x\": \"NONE\",\n",
    "}\n",
    "\n",
    "# UPOS tag codes dictionary. refer to the following:\n",
    "# https://universaldependencies.org/u/pos/\n",
    "# https://github.com/explosion/spaCy/blob/abb0ab109d33d2deaa6155a61fad649a25472f9c/spacy/glossary.py#L22\n",
    "upos_code_dict = {\n",
    "    \"adj\": \"a\",\n",
    "    \"adp\": \"x\",\n",
    "    \"adv\": \"r\",\n",
    "    \"aux\": \"x\",\n",
    "    \"conj\": \"x\",\n",
    "    \"cconj\": \"x\",\n",
    "    \"det\": \"x\",\n",
    "    \"intj\": \"x\",\n",
    "    \"noun\": \"n\",\n",
    "    \"num\": \"x\",\n",
    "    \"part\": \"x\",\n",
    "    \"pron\": \"n\",\n",
    "    \"propn\": \"n\",\n",
    "    \"punct\": \"x\",\n",
    "    \"sconj\": \"x\",\n",
    "    \"sym\": \"x\",\n",
    "    \"verb\": \"v\",\n",
    "    \"x\": \"x\",\n",
    "    \"eol\": \"x\",\n",
    "    \"space\": \"x\",\n",
    "    \"default\": \"x\",\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD Preprocessing\n",
    "\n",
    "We read the MRD file as a CSV file.\n",
    "\n",
    "The CSV file should have a row for every sense of every word.\n",
    "\n",
    "It should contain the words and definitions in its columns.\n",
    "\n",
    "Different senses of the same word should be sorted by frequency of use, and part of speech (POS) tags.\n",
    "\n",
    "The particular dictionary used in this implementation is based on a sqlite database of the wordnet, obtained from:\n",
    "\n",
    "http://sqlunet.sourceforge.net/\n",
    "\n",
    "Specifically:\n",
    "\n",
    "https://sourceforge.net/projects/sqlunet/files/6.0.0/sqlite/XX/sqlite-6.0.0-XX-all.zip\n",
    "\n",
    "Which contains the file:\n",
    "\n",
    "sqlite-XX.db\n",
    "\n",
    "Which was further processed with the following SQL query to obtain a CSV file:\n",
    "\n",
    "```sql\n",
    "DROP VIEW IF EXISTS dict_compact;\n",
    "\n",
    "CREATE VIEW dict_compact AS SELECT\n",
    "words.*, casedwords.cased, postypes.posname, lexdomains.lexdomainname, synsets.definition\n",
    "FROM words\n",
    "LEFT JOIN casedwords USING (wordid)\n",
    "LEFT JOIN senses USING (wordid)\n",
    "LEFT JOIN synsets USING (synsetid)\n",
    "LEFT JOIN lexdomains USING (lexdomainid)\n",
    "LEFT JOIN postypes USING (pos)\n",
    "ORDER BY\n",
    "words.wordid ASC,\n",
    "postypes.pos ASC,\n",
    "senses.tagcount DESC,\n",
    "senses.sensenum ASC;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The main function.\"\"\"\n",
    "logger.info(\"Reading dataframe from csv...\")\n",
    "orig_dict_df = pd.read_csv(\n",
    "    input_data_dir + \"dict_compact_wordnet.csv\", encoding='utf-8')\n",
    "\n",
    "log_obj(logging.INFO, orig_dict_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLP module.\n",
    "nlp = spacy.load('en_core_web_sm')  # for efficiency.\n",
    "# nlp = spacy.load('en_core_web_trf')  # for accuracy.\n",
    "\n",
    "# Select necessary NLP pipe components.\n",
    "# default selection takes around 6 mins.\n",
    "# custom selection takes around 1.5 mins.\n",
    "nlp.select_pipes(enable=['tagger', 'attribute_ruler', 'lemmatizer'])\n",
    "logger.info(f\"{nlp.pipe_names = }\")\n",
    "logger.info(f\"{nlp.analyze_pipes() = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The following is a cool example of adding a custom NLP pipe to spacy.\n",
    "# # However, it turns out what I want can be accomplished much more simply.\n",
    "\n",
    "# #### Add a custom NLP pipe for lowercasing.\n",
    "# # Add the attribute to store the NLP pi[e] result.\n",
    "# if not spacy.tokens.Token.has_extension('lower_'):\n",
    "#     spacy.tokens.Token.set_extension('lower_', default='')\n",
    "\n",
    "# # Define the actual NLP pipe.\n",
    "# @spacy.language.Language.component('lowercaser')\n",
    "# def lowercaser(doc):\n",
    "#    # Do something to the doc here\n",
    "#    for token in doc:\n",
    "#        token._.lower_ = token.lemma_.lower()\n",
    "#    return doc\n",
    "\n",
    "# # Add the pipe to end of the pipeline.\n",
    "# nlp.add_pipe('lowercaser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply spacy preprocessing.\n",
    "# Note: Spacy lemmatization keeps capitals when not using tagger. Hence, do not disable it.\n",
    "# Remove null term rows. words like \"NaN\" are troublesome, so they are dropped.\n",
    "dict_df = orig_dict_df.dropna(subset=[term_colname])\n",
    "# Use smaller dataset for testing (optional).\n",
    "# dict_df = dict_df.iloc[:50000, :]  # todo: remove.\n",
    "\n",
    "# Apply NLP preprocessing to dictionary terms.\n",
    "preproc_term_docs = [\n",
    "    [token.lemma_.lower()\n",
    "        for token in doc if (token.is_alpha and not token.is_stop)]\n",
    "    for doc in nlp.pipe(dict_df[term_colname])\n",
    "]\n",
    "preproc_term_docs = ['_'.join(doc) for doc in preproc_term_docs]\n",
    "# Apply NLP preprocessing to dictionary definitions.\n",
    "preproc_defn_docs = [\n",
    "    [token.lemma_.lower()\n",
    "        for token in doc if (token.is_alpha and not token.is_stop)]\n",
    "    for doc in nlp.pipe(dict_df[defn_colname])\n",
    "]\n",
    "# Extract the postag information as well. this is useful for WSD.\n",
    "preproc_postags = dict_df[pos_colname]\n",
    "\n",
    "# ## The following variants were added to facilitate PoS tagging for definitions.\n",
    "# ## Tagging results were not very good, so it was removed later.\n",
    "# # Apply NLP preprocessing to dictionary definitions.\n",
    "# preproc_defn_docs = [\n",
    "#     [(token.lemma_.lower(),\n",
    "#         upos_code_dict.get(token.pos_.lower(), upos_code_dict[\"default\"]))\n",
    "#         for token in doc if (token.is_alpha and not token.is_stop)]\n",
    "#     for doc in nlp.pipe(dict_df[defn_colname])\n",
    "# ]\n",
    "# # Extract the postag information as well. this is useful for WSD.\n",
    "# preproc_postags = dict_df[pos_colname].apply(\n",
    "#     lambda x: pos_code_dict.get(x, pos_code_dict[\"default\"]))\n",
    "\n",
    "# Combine terms, postags, and definitions in a table.\n",
    "preproc_dict_table = [[term_doc, posname, defn_doc] for term_doc, posname, defn_doc\n",
    "                      in zip(preproc_term_docs, preproc_postags, preproc_defn_docs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary dataframe from preprocessed dictionary table.\n",
    "preproc_dict_df = pd.DataFrame(copy.deepcopy(preproc_dict_table), columns=[\n",
    "                               term_colname, pos_colname, defn_colname])\n",
    "# Apply dataframe preprocessing.\n",
    "# Remove empty term and definition rows.\n",
    "preproc_dict_df = preproc_dict_df[preproc_dict_df[term_colname].astype(bool)]\n",
    "preproc_dict_df = preproc_dict_df[preproc_dict_df[defn_colname].astype(bool)]\n",
    "preproc_dict_df = preproc_dict_df.reset_index(drop=True)\n",
    "\n",
    "# ## the following two steps are now removed, since we will be using WSD now.\n",
    "# # Keep first n term rows for each term, if it has multiple definitions.\n",
    "# rows_to_keep_per_term = 3\n",
    "# preproc_dict_df = preproc_dict_df.groupby(preproc_dict_df[term_colname]).head(rows_to_keep_per_term)\n",
    "# preproc_dict_df = preproc_dict_df.reset_index()  # not necessary if groupby is done via index column.\n",
    "# # Combine duplicate term rows.\n",
    "# preproc_dict_df = preproc_dict_df.groupby(preproc_dict_df[term_colname]).agg({defn_colname: 'sum'})\n",
    "# preproc_dict_df = preproc_dict_df.reset_index()  # not necessary if groupby is done via index column.\n",
    "\n",
    "# add sense id column to preprocessed dictionary dataframe. useful for WSD.\n",
    "preproc_dict_df.insert(loc=preproc_dict_df.columns.get_loc(defn_colname), column=senseid_colname,\n",
    "                       value=preproc_dict_df.groupby(preproc_dict_df[term_colname]).cumcount())\n",
    "\n",
    "# Print and save preprocessed dictionary dataframe.\n",
    "log_obj(logging.INFO, preproc_dict_df)\n",
    "preproc_dict_df.to_pickle(interm_data_dir + \"preproc_dict_df.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dict_df = pd.read_pickle(interm_data_dir + \"preproc_dict_df.pkl\")\n",
    "\n",
    "preproc_dict_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preproc_dict_df[preproc_dict_df[term_colname] == \"gain\"])\n",
    "display(preproc_dict_df[preproc_dict_df[term_colname] == \"signal\"])\n",
    "\n",
    "display(preproc_dict_df[preproc_dict_df[term_colname] == \"gain\"]\n",
    "        .iloc[2].definition)\n",
    "display(preproc_dict_df[preproc_dict_df[term_colname] == \"signal\"]\n",
    "        .iloc[2].definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synset_name(row):\n",
    "    return f\"{row[term_colname]}.{pos_code_dict[row[pos_colname]]}.{row[senseid_colname]:02d}\"\n",
    "\n",
    "\n",
    "def custom_lesk(row):\n",
    "    if row._name % 1000 == 0:\n",
    "        print(f\"custom_lesk: processing row: {row._name}...\")\n",
    "\n",
    "    context = row[defn_colname]\n",
    "    context_wsd = []\n",
    "\n",
    "    # print(f\"{context = }\")\n",
    "\n",
    "    for token in context:\n",
    "        synsets = preproc_dict_df[preproc_dict_df[term_colname] == token]\n",
    "\n",
    "        if synsets.empty:\n",
    "            continue\n",
    "\n",
    "        # # nltk lesk implementation.\n",
    "        # _, sense = max(\n",
    "        #     ((len(set(context) & set(ss[defn_colname])), ss)\n",
    "        #      for _, ss in synsets.iterrows()),\n",
    "        #     key=lambda x: (x[0], -x[1][senseid_colname])\n",
    "        # )\n",
    "\n",
    "        # pandas lesk implementation. hopefully will be slightly faster.\n",
    "        sense = synsets.loc[synsets.apply(lambda x: len(\n",
    "            set(context) & set(x[defn_colname])), axis=1).idxmax(), :]\n",
    "\n",
    "        context_wsd.append(\n",
    "            f\"{token}.{pos_code_dict[sense[pos_colname]]}.{sense[senseid_colname]:02d}\")\n",
    "\n",
    "    # print(f\"{context_wsd = }\")\n",
    "\n",
    "    return context_wsd\n",
    "\n",
    "\n",
    "# mrd_wsd_slice = slice(74661, 74673)\n",
    "# mrd_wsd_slice = slice(0, 100)\n",
    "mrd_wsd_slice = slice(None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply WSD to preprocessed dictionary dataframe.\n",
    "# Note that this procedure takes time!\n",
    "wsd_dict_df = preproc_dict_df.copy()\n",
    "\n",
    "wsd_dict_df.loc[mrd_wsd_slice, defn_colname] = wsd_dict_df.loc[mrd_wsd_slice, :].apply(\n",
    "    lambda x: custom_lesk(x), axis=1)\n",
    "wsd_dict_df.loc[mrd_wsd_slice, term_colname] = wsd_dict_df.loc[mrd_wsd_slice, :].apply(\n",
    "    lambda x: get_synset_name(x), axis=1)\n",
    "\n",
    "wsd_dict_df = wsd_dict_df.drop(columns=[pos_colname, senseid_colname])\n",
    "\n",
    "# log_obj(logging.INFO, wsd_dict_df)\n",
    "wsd_dict_df.to_pickle(interm_data_dir + \"wsd_dict_df.pkl\")\n",
    "\n",
    "wsd_dict_df.loc[mrd_wsd_slice, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple sanity check.\n",
    "\n",
    "wsd_dict_df = pd.read_pickle(interm_data_dir + \"wsd_dict_df.pkl\")\n",
    "\n",
    "display(wsd_dict_df[wsd_dict_df[term_colname].str.startswith(\"gain.\")])\n",
    "display(wsd_dict_df[wsd_dict_df[term_colname].str.startswith(\"signal.\")])\n",
    "\n",
    "display(wsd_dict_df[wsd_dict_df[term_colname].str.startswith(\"signal.\")]\n",
    "        .iloc[2].definition)\n",
    "display(wsd_dict_df[wsd_dict_df[term_colname].str.startswith(\"field.\")]\n",
    "        .iloc[14].definition)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD2SKB_TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_dict_df = pd.read_pickle(interm_data_dir + \"wsd_dict_df.pkl\")\n",
    "\n",
    "log_obj(logging.INFO, wsd_dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get docs and titles.\n",
    "titles = copy.deepcopy(wsd_dict_df[term_colname].to_list())\n",
    "docs = copy.deepcopy(wsd_dict_df[defn_colname].to_list())\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts.\n",
    "# Note that our docs are already preprocessed and tokenized.\n",
    "# Hence, we replace the analyzer with the identity function.\n",
    "tcv = skl_feat_text.CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "preproc_dict_mtx = tcv.fit_transform(docs)\n",
    "defn_words = tcv.get_feature_names_out()\n",
    "\n",
    "# Reduce memory usage in matrix with casting.\n",
    "preproc_dict_mtx = preproc_dict_mtx.astype(\"uint16\")\n",
    "\n",
    "# Construct index objects.\n",
    "preproc_dict_words = pd.Index(titles)\n",
    "preproc_defn_words = pd.Index(defn_words)\n",
    "\n",
    "# Create dataframe.\n",
    "preproc_dict_mtx_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    preproc_dict_mtx, index=preproc_dict_words, columns=preproc_defn_words)\n",
    "\n",
    "# Print the dictionary matrix dataframe.\n",
    "log_obj(logging.INFO, preproc_dict_mtx_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple sanity check.\n",
    "test_word = 'tweet.'\n",
    "\n",
    "preproc_dict_mtx_df.loc[preproc_dict_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(preproc_dict_mtx_df.columns[x.values > 0]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 1 documents, or more than 4% of the documents.\n",
    "# The following filter passes all tokens for mrd2skb. May tune it later.\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.04)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print(f\"Number of unique tokens: {len(dictionary)}\")\n",
    "print(f\"Number of documents: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corpus data to disk before training.\n",
    "corpus_dir = interm_data_dir + 'corpus/'\n",
    "pathlib.Path(corpus_dir).mkdir(parents=True, exist_ok=True)\n",
    "gensim.corpora.MmCorpus.serialize(corpus_dir + 'corpus.gensim', corpus)\n",
    "dictionary.save(corpus_dir + 'dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gensim.corpora.MmCorpus(corpus_dir + 'corpus.gensim')\n",
    "dictionary = gensim.corpora.Dictionary.load(corpus_dir + 'dictionary.gensim')\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training hyperparameters.\n",
    "hyperparams = {\n",
    "    \"top_model_name\": gensim.models.LdaModel.__name__,\n",
    "    \"num_topics\": 100,  # sememe set size is approximately 20 * num_topics\n",
    "    \"sememe_annotation_thrs\": 0,  # unchanged, kept for compatibility reasons.\n",
    "}\n",
    "\n",
    "# Set the common arguments for each model.\n",
    "top_model_args = {\n",
    "    \"corpus\": corpus,\n",
    "    \"id2word\": dictionary.id2token,\n",
    "    \"num_topics\": hyperparams[\"num_topics\"],\n",
    "    # uncomment the following for LDA, comment out for pLSA.\n",
    "    \"alpha\": \"auto\",\n",
    "    \"eta\": \"auto\",\n",
    "}\n",
    "\n",
    "# Add model specific arguments, if applicable.\n",
    "top_model_module = None\n",
    "match hyperparams[\"top_model_name\"]:\n",
    "    case gensim.models.Nmf.__name__:\n",
    "        top_model_module = gensim.models.Nmf\n",
    "        pass\n",
    "    case gensim.models.LsiModel.__name__:\n",
    "        top_model_module = gensim.models.LsiModel\n",
    "        pass\n",
    "    case gensim.models.LdaModel.__name__:\n",
    "        top_model_module = gensim.models.LdaModel\n",
    "        \n",
    "        if top_model_args.get(\"alpha\", \"symmetric\") == \"symmetric\" and top_model_args.get(\"eta\", \"symmetric\") == \"symmetric\":\n",
    "            hyperparams['top_model_name'] = \"PLsaModel\"\n",
    "\n",
    "        pass\n",
    "    case gensim.models.EnsembleLda.__name__:\n",
    "        # top_model_module = gensim.models.EnsembleLda  # UNUSED\n",
    "        pass\n",
    "    case gensim.models.HdpModel.__name__:\n",
    "        # top_model_module = gensim.models.HdpModel  # UNUSED\n",
    "        top_model_args.pop('num_topics', None)  # HDP does not take num_topics.\n",
    "        pass\n",
    "    case _:\n",
    "        logger.info(f\"Unknown topic model module: {hyperparams['top_model_name']}\")\n",
    "\n",
    "# Set directory to save the model.\n",
    "model_dir = interm_data_dir + f'{hyperparams[\"top_model_name\"]}_{hyperparams[\"num_topics\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a topic model.\n",
    "with timer_con():\n",
    "    logger_root.setLevel(logging.DEBUG)\n",
    "    logger_root.addHandler(log_fh)\n",
    "    \n",
    "    model = top_model_module(**top_model_args)\n",
    "\n",
    "    logger_root.removeHandler(log_fh)\n",
    "\n",
    "# Save model to disk.\n",
    "pathlib.Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "model.save(model_dir + 'model.gensim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model.\n",
    "logger_root.removeHandler(log_fh)\n",
    "model = top_model_module.load(model_dir + 'model.gensim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the sememe set.\n",
    "sememe_set = set()\n",
    "for topic_id in range(model.num_topics):\n",
    "    topk = model.show_topic(topic_id, topn=20)\n",
    "    topk_words = [ w for w, _ in topk ]\n",
    "    \n",
    "    # print(f\"{topic_id}: {topk_words}\")\n",
    "    sememe_set.update(topk_words)\n",
    "\n",
    "print(f\"{len(sememe_set) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the SKB matrix.\n",
    "mrd2skb_mtx_df = preproc_dict_mtx_df.copy()\n",
    "mrd2skb_mtx_df = mrd2skb_mtx_df[mrd2skb_mtx_df.columns.intersection(sememe_set)]\n",
    "\n",
    "# Print and save results.\n",
    "log_obj(logging.INFO, mrd2skb_mtx_df)\n",
    "mrd2skb_mtx_df.to_pickle(output_data_dir + \"mrd2skb_mtx_df.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD2SKB Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all MRD2SKB methods, mrd2skb_mtx_df should be generated.\n",
    "# then, the following portions of the code will stay the same.\n",
    "\n",
    "mrd2skb_mtx_df = pd.read_pickle(output_data_dir + \"mrd2skb_mtx_df.pkl\")\n",
    "\n",
    "log_obj(logging.INFO, mrd2skb_mtx_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some simple sanity checks.\n",
    "test_word = 'tweet.'\n",
    "\n",
    "test_annotation = mrd2skb_mtx_df.loc[mrd2skb_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(mrd2skb_mtx_df.columns[x.values > 0]), axis=1)\n",
    "\n",
    "print(test_annotation)\n",
    "\n",
    "test_word = 'screenwriter.'\n",
    "\n",
    "test_annotation = mrd2skb_mtx_df.loc[mrd2skb_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(mrd2skb_mtx_df.columns[x.values > 0]), axis=1)\n",
    "\n",
    "print(test_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrd2skb output formatting functions.\n",
    "\n",
    "def parse_sense_str(sense_str: str) -> dict:\n",
    "    sense_dict_keys = [term_colname, pos_colname, senseid_colname]\n",
    "    return dict(zip(sense_dict_keys, sense_str.rsplit('.', 2)))\n",
    "\n",
    "\n",
    "@timer_dec\n",
    "def gen_mrd2skb_list_df(mrd2skb_mtx_df, hyperparams):\n",
    "    mrd2skb_bin_df = (\n",
    "        mrd2skb_mtx_df > hyperparams['sememe_annotation_thrs'])\n",
    "    mrd2skb_list_df = mrd2skb_bin_df.dot(\n",
    "        mrd2skb_bin_df.columns + ' ').str.rstrip().str.split()\n",
    "    return mrd2skb_list_df\n",
    "\n",
    "\n",
    "def generate_txt_outputs(mrd2skb_mtx_df, mrd2skb_list_df):\n",
    "    # save \"mrd2skb_sememes.txt\".\n",
    "    with open(output_data_dir + \"mrd2skb_sememes.txt\", 'w') as fp:\n",
    "        sememes = (parse_sense_str(term)[term_colname]\n",
    "                   for term in mrd2skb_mtx_df.columns)\n",
    "        [fp.write(f\"{sememe}\\n\") for sememe in sorted(set(sememes))]\n",
    "\n",
    "    # merge multiple sense definitions of the same terms.\n",
    "    mrg_list_df = mrd2skb_list_df.copy()\n",
    "    mrg_list_df.index = mrg_list_df.index.map(\n",
    "        lambda x: parse_sense_str(x)[term_colname])\n",
    "    mrg_list_df = mrg_list_df.map(\n",
    "        lambda x: [parse_sense_str(term)[term_colname] for term in x])\n",
    "    mrg_list_df = mrg_list_df.groupby(mrg_list_df.index).sum().apply(\n",
    "        lambda x: sorted(set(x), key=x.index))\n",
    "\n",
    "    # save \"mrd2skb_skb.txt\".\n",
    "    with open(output_data_dir + \"mrd2skb_skb.txt\", 'w') as fp:\n",
    "        for key, value in mrg_list_df.to_dict().items():\n",
    "            if len(value) > 0:\n",
    "                term = parse_sense_str(key)[term_colname]\n",
    "                sememe_set = ' '.join(parse_sense_str(sememe)[term_colname]\n",
    "                                      for sememe in value)\n",
    "                fp.write(f\"{term}\\n{sememe_set}\\n\")\n",
    "\n",
    "\n",
    "def generate_npy_outputs(mrd2skb_mtx_df, mrd2skb_list_df):\n",
    "    # save \"mrd2skb_sememes.npy\".\n",
    "    sememes = (parse_sense_str(term)[term_colname]\n",
    "               for term in mrd2skb_mtx_df.columns)\n",
    "    sememes_np = np.asarray(sorted(set(sememes)))\n",
    "    np.save(output_data_dir + \"mrd2skb_sememes.npy\", sememes_np)\n",
    "\n",
    "    # save \"mrd2skb_valid_words.npy\".\n",
    "    valid_words = (parse_sense_str(term)[term_colname]\n",
    "                   for term in mrd2skb_mtx_df.index)\n",
    "    valid_words_np = np.asarray(sorted(set(valid_words)))\n",
    "    np.save(output_data_dir + \"mrd2skb_valid_words.npy\", valid_words_np)\n",
    "\n",
    "    # save \"mrd2skb_dict.npy\".\n",
    "    mrg_list_df = mrd2skb_list_df.copy()\n",
    "\n",
    "    mrg_list_df = mrg_list_df.map(\n",
    "        lambda x: [parse_sense_str(term)[term_colname] for term in x])\n",
    "\n",
    "    mrg_list_df_idx = mrd2skb_list_df.index.str.rsplit(\n",
    "        '.', n=2).map(lambda x: x[0])\n",
    "\n",
    "    mrg_list_df = mrg_list_df.groupby(mrg_list_df_idx).apply(\n",
    "        lambda x: [(pos_code_inv_dict[sense.rsplit('.', 2)[1]], set(defn)) for sense, defn in zip(x.index, x)])\n",
    "\n",
    "    mrd2skb_dict = mrg_list_df.to_dict()\n",
    "    np.save(output_data_dir + \"mrd2skb_dict.npy\", mrd2skb_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate skb list dataframe with binary sememe annotations.\n",
    "mrd2skb_list_df = gen_mrd2skb_list_df(mrd2skb_mtx_df, hyperparams)\n",
    "\n",
    "# Print and save results.\n",
    "log_obj(logging.INFO, mrd2skb_list_df)\n",
    "mrd2skb_list_df.to_pickle(output_data_dir + \"mrd2skb_list_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrd2skb_list_df = pd.read_pickle(output_data_dir + \"mrd2skb_list_df.pkl\")\n",
    "\n",
    "log_obj(logging.INFO, mrd2skb_list_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and save skb outputs. these files are used in ThuNLP DictSKB code.\n",
    "generate_txt_outputs(mrd2skb_mtx_df, mrd2skb_list_df)\n",
    "generate_npy_outputs(mrd2skb_mtx_df, mrd2skb_list_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for the skb file outputs.\n",
    "mrd2skb_sememes = np.load(output_data_dir + \"mrd2skb_sememes.npy\")\n",
    "mrd2skb_valid_words = np.load(output_data_dir + \"mrd2skb_valid_words.npy\")\n",
    "mrd2skb_dict = np.load(\n",
    "    output_data_dir + \"mrd2skb_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "print(f\"{mrd2skb_sememes.size = }\")\n",
    "# print(f\"{mrd2skb_sememes = }\")\n",
    "print(f\"{type(mrd2skb_sememes) = }\")\n",
    "\n",
    "print(f\"{mrd2skb_valid_words.size = }\")\n",
    "# print(f\"{mrd2skb_valid_words = }\")\n",
    "print(f\"{type(mrd2skb_valid_words) = }\")\n",
    "\n",
    "print(f\"{len(mrd2skb_dict) = }\")\n",
    "print(f\"{mrd2skb_dict['tweet'] = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup functions.\n",
    "\n",
    "def get_hparam_hash(hyperparams):\n",
    "    hparams_json = json.dumps(hyperparams).encode(\"utf-8\")\n",
    "    hparams_hash = hashlib.md5(hparams_json).hexdigest()\n",
    "    return hparams_hash\n",
    "\n",
    "\n",
    "def backup_data(hyperparams):\n",
    "    hparam_str = \"_\".join(\n",
    "        [f\"x{idx}={val}\" for idx, (key, val) in enumerate(hyperparams.items())])\n",
    "    cur_backup_dirname = f\"mrd2skb_top_bkp_{hparam_str}/\"\n",
    "    cur_backup_dir = backup_data_dir + cur_backup_dirname\n",
    "    pathlib.Path(cur_backup_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(cur_backup_dir + 'hparams.json', 'w') as fp:\n",
    "        json.dump(hyperparams, fp, ensure_ascii=False)\n",
    "\n",
    "    shutil.copytree(interm_data_dir, cur_backup_dir +\n",
    "                    \"interm/\", dirs_exist_ok=True)\n",
    "    shutil.copytree(output_data_dir, cur_backup_dir +\n",
    "                    \"output/\", dirs_exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup mrd2skb data for the current session.\n",
    "backup_data(hyperparams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD2SKB Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations\n",
    "\n",
    "To perform the evaluations, download the DictSKB repo from Github: https://github.com/thunlp/DictSKB\n",
    "\n",
    "Prepare an appropriate conda environment for each evaluation task, as per documented in the repo itself.\n",
    "\n",
    "Then, perform the following file substitutions to obtain the evaluation results for MRD2SKB:\n",
    "\n",
    "- adversarial_attack/core_sememe_dict.npy <- mrd2skb_dict.npy\n",
    "- consistency_check/core_sememe_dict.npy <- mrd2skb_dict.npy\n",
    "- lm_sdlm/sememe_dict.uncased.npy <- mrd2skb_dict.npy\n",
    "\n",
    "- consistency_check/dict_sememes.npy <- mrd2skb_sememes.npy\n",
    "- consistency_check/dict_sememes.npy <- mrd2skb_valid_words.npy\n",
    "\n",
    "- nli/sememe_dict.txt <- mrd2skb_skb.txt\n",
    "- nli/sememes.txt <- mrd2skb_sememes.txt\n",
    "\n",
    "In order not to change the code in DictSKB, keep the names of the files the same, just overwrite the contents.\n",
    "\n",
    "Then, perform the evaluations as described in the DictSKB repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1ce64c2b35e2ec82aa358cee80e6b28a966da108a18834680f418c57a012dd8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
