{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRD2SKB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Preliminary setup.\n",
    "\n",
    "# ## Library imports.\n",
    "\n",
    "# Standard libraries.\n",
    "import contextlib\n",
    "import copy\n",
    "import functools\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import inspect\n",
    "import operator\n",
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# External libraries.\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd  # check faster alternatives?\n",
    "# https://www.datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray\n",
    "\n",
    "# ML specific libraries.\n",
    "import gensim\n",
    "import spacy\n",
    "import sklearn.feature_extraction.text as skl_feat_text\n",
    "\n",
    "# Following are some libraries for fast gpu computations.\n",
    "# import dask.bag as db\n",
    "# import dask.array as da\n",
    "# import dask.dataframe as dd\n",
    "# import cupyx as cpx\n",
    "# import cupy as cp\n",
    "# import numba\n",
    "# import swifter\n",
    "\n",
    "# Jupyter notebook libraries.\n",
    "from IPython.display import display  # enable this when converting to a script.\n",
    "from IPython import get_ipython\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "# ## General path configurations.\n",
    "\n",
    "# Get script file absolute path.\n",
    "file_abspath = None\n",
    "if '__file__' not in globals():\n",
    "    # We are in a .ipynb notebook, and presumably running in vscode.\n",
    "    ip = get_ipython()\n",
    "    file_abspath = ip.user_ns.get('__vsc_ipynb_file__', None)\n",
    "else:\n",
    "    # We are in a .py script.\n",
    "    file_abspath = os.path.abspath(__file__)\n",
    "\n",
    "file_basename = os.path.basename(file_abspath)\n",
    "file_dirname = os.path.dirname(file_abspath)\n",
    "file_name, file_ext = os.path.splitext(file_basename)\n",
    "\n",
    "\n",
    "# ## Preconfigurations for module logger.\n",
    "# Get the root logger, this is sometimes useful.\n",
    "logger_root = logging.getLogger()\n",
    "# Create logger with current module.\n",
    "if 'logger' not in vars():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    log_formatter = logging.Formatter(\"[%(levelname)s]: %(message)s\")\n",
    "    log_formatter_verbose = logging.Formatter(\n",
    "        \"[%(levelname)s]: %(asctime)s; File %(filename)s, line %(lineno)d, in %(funcName)s: %(message)s\")\n",
    "    # Create console log handler.\n",
    "    log_ch = logging.StreamHandler()\n",
    "    log_ch.setFormatter(log_formatter)\n",
    "    logger.addHandler(log_ch)\n",
    "    # Create file log handler.\n",
    "    log_fh = logging.FileHandler(f\"{file_abspath}.log\")\n",
    "    log_fh.setFormatter(log_formatter_verbose)\n",
    "    # logger.addHandler(log_fh)\n",
    "\n",
    "# Set log handler levels.\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# log_ch.setLevel(logging.INFO)\n",
    "# log_fh.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def log_obj(level, obj):\n",
    "    \"\"\"Log an object, using ipython display function.\"\"\"\n",
    "    # Obtain the name of the object.\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    obj_name = [obj_name for obj_name, obj_val in callers_local_vars\n",
    "                if obj_val is obj][0]\n",
    "    # log and display the object if log level permits.\n",
    "    if level >= logger.level:\n",
    "        logger.log(level, f\"{obj_name} = \")\n",
    "        display(obj)\n",
    "\n",
    "# ## GPU configurations.\n",
    "\n",
    "# Check GPU availability for supported libraries.\n",
    "# logger.debug(f\"{torch.cuda.is_available() = }\")\n",
    "# logger.debug(f\"{torch.cuda.get_device_name(0) = }\")\n",
    "# logger.debug(f\"{tf.config.list_physical_devices('GPU') = }\")\n",
    "\n",
    "# Check BLAS and LAPACK availability for supported libraries.\n",
    "# logger.debug(f\"{np.show_config() = }\")\n",
    "\n",
    "\n",
    "# Use GPU if applicable.\n",
    "logger.debug(f\"{spacy.prefer_gpu() = }\")\n",
    "\n",
    "\n",
    "# ## Timer configurations.\n",
    "\n",
    "\n",
    "def timer_dec(func):\n",
    "    \"\"\"Print the runtime of the decorated function.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        \"\"\"Wrap the function and time it.\"\"\"\n",
    "        func_name = repr(func.__name__)\n",
    "        logger.debug(f\"{func_name} Start\")\n",
    "        start_time = time.perf_counter()\n",
    "        value = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        run_time = end_time - start_time\n",
    "        logger.debug(f\"{func_name} End\")\n",
    "        logger.info(f\"{func_name} runtime: {run_time:.6f} secs.\")\n",
    "        return value\n",
    "    return wrapper_timer\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timer_con():\n",
    "    \"\"\"Print the runtime of code block in the managed context.\"\"\"\n",
    "    logger.debug(f\"Timer Start\")\n",
    "    start_time = time.perf_counter()\n",
    "    yield\n",
    "    end_time = time.perf_counter()\n",
    "    run_time = end_time - start_time\n",
    "    logger.debug(f\"Timer End\")\n",
    "    logger.info(f\"Runtime: {run_time:.6f} secs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Project specific configurations.\n",
    "\n",
    "\"\"\" MRD2SKB Project Directory Structure\n",
    "mrd2skb/\n",
    "    data/\n",
    "        input/\n",
    "            dict_compact_wordnet.csv\n",
    "        interm/\n",
    "            preproc_dict_df.pkl\n",
    "            wsd_dict_df.pkl\n",
    "        output/\n",
    "            mrd2skb_mtx_df.pkl\n",
    "            mrd2skb_list_df.pkl\n",
    "            mrd2skb_sememes.txt\n",
    "            mrd2skb_skb.txt\n",
    "            mrd2skb_dict.npy\n",
    "            mrd2skb_sememes.npy\n",
    "            mrd2skb_valid_words.npy\n",
    "            models/\n",
    "                ...\n",
    "        backup/\n",
    "            mrd2skb_kro_bkp_x0=xx_x1=xx_x2=xx/\n",
    "                interm/\n",
    "                    ...\n",
    "                output/\n",
    "                    ...\n",
    "            mrd2skb_top_bkp_x0=xx_x1=xx_x2=xx/\n",
    "                interm/\n",
    "                    ...\n",
    "                output/\n",
    "                    ...\n",
    "    src/\n",
    "        mrd2skb_kro.ipynb\n",
    "        mrd2skb_top.ipynb\n",
    "\"\"\"\n",
    "\n",
    "os.chdir(file_dirname)\n",
    "\n",
    "# ## Project specific paths and directories.\n",
    "# Relative paths of input output directories.\n",
    "prj_root_dir = \"../\"\n",
    "prj_data_dir = prj_root_dir + \"data/\"\n",
    "input_data_dir = prj_data_dir + \"input/\"\n",
    "interm_data_dir = prj_data_dir + \"interm/\"\n",
    "output_data_dir = prj_data_dir + \"output/\"\n",
    "backup_data_dir = prj_data_dir + \"backup/\"\n",
    "\n",
    "# Create directories if they do not exist.\n",
    "pathlib.Path(interm_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(output_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(backup_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ## Project specific variables.\n",
    "# Column names for terms and definitions from dictionary dataset.\n",
    "term_colname = \"lemma\"\n",
    "pos_colname = \"posname\"\n",
    "senseid_colname = \"senseid\"\n",
    "defn_colname = \"definition\"\n",
    "\n",
    "# PoS tag codes dictionary. depends on the MRD.\n",
    "# For WordNet, see ss_type under https://wordnet.princeton.edu/documentation/wndb5wn.\n",
    "# The following code may be useful for determining these values:\n",
    "# preproc_dict_df[pos_colname].unique()\n",
    "pos_code_dict = {\n",
    "    \"noun\": \"n\",\n",
    "    \"verb\": \"v\",\n",
    "    \"adjective\": \"a\",\n",
    "    \"adjective satellite\": \"a\",  # this is actually \"s\", but we prefer \"a\".\n",
    "    \"adverb\": \"r\",\n",
    "    \"default\": \"x\",\n",
    "}\n",
    "\n",
    "# The following is useful for generating the output skb files.\n",
    "pos_code_inv_dict = {\n",
    "    \"n\": \"noun\",\n",
    "    \"v\": \"verb\",\n",
    "    \"a\": \"adjective\",\n",
    "    \"r\": \"adverb\",\n",
    "    \"x\": \"NONE\",\n",
    "}\n",
    "\n",
    "# UPOS tag codes dictionary. refer to the following:\n",
    "# https://universaldependencies.org/u/pos/\n",
    "# https://github.com/explosion/spaCy/blob/abb0ab109d33d2deaa6155a61fad649a25472f9c/spacy/glossary.py#L22\n",
    "upos_code_dict = {\n",
    "    \"adj\": \"a\",\n",
    "    \"adp\": \"x\",\n",
    "    \"adv\": \"r\",\n",
    "    \"aux\": \"x\",\n",
    "    \"conj\": \"x\",\n",
    "    \"cconj\": \"x\",\n",
    "    \"det\": \"x\",\n",
    "    \"intj\": \"x\",\n",
    "    \"noun\": \"n\",\n",
    "    \"num\": \"x\",\n",
    "    \"part\": \"x\",\n",
    "    \"pron\": \"n\",\n",
    "    \"propn\": \"n\",\n",
    "    \"punct\": \"x\",\n",
    "    \"sconj\": \"x\",\n",
    "    \"sym\": \"x\",\n",
    "    \"verb\": \"v\",\n",
    "    \"x\": \"x\",\n",
    "    \"eol\": \"x\",\n",
    "    \"space\": \"x\",\n",
    "    \"default\": \"x\",\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD Preprocessing\n",
    "\n",
    "We read the MRD file as a CSV file.\n",
    "\n",
    "The CSV file should have a row for every sense of every word.\n",
    "\n",
    "It should contain the words and definitions in its columns.\n",
    "\n",
    "Different senses of the same word should be sorted by frequency of use, and part of speech (POS) tags.\n",
    "\n",
    "The particular dictionary used in this implementation is based on a sqlite database of the wordnet, obtained from:\n",
    "\n",
    "http://sqlunet.sourceforge.net/\n",
    "\n",
    "Specifically:\n",
    "\n",
    "https://sourceforge.net/projects/sqlunet/files/6.0.0/sqlite/XX/sqlite-6.0.0-XX-all.zip\n",
    "\n",
    "Which contains the file:\n",
    "\n",
    "sqlite-XX.db\n",
    "\n",
    "Which was further processed with the following SQL query to obtain a CSV file:\n",
    "\n",
    "```sql\n",
    "DROP VIEW IF EXISTS dict_compact;\n",
    "\n",
    "CREATE VIEW dict_compact AS SELECT\n",
    "words.*, casedwords.cased, postypes.posname, lexdomains.lexdomainname, synsets.definition\n",
    "FROM words\n",
    "LEFT JOIN casedwords USING (wordid)\n",
    "LEFT JOIN senses USING (wordid)\n",
    "LEFT JOIN synsets USING (synsetid)\n",
    "LEFT JOIN lexdomains USING (lexdomainid)\n",
    "LEFT JOIN postypes USING (pos)\n",
    "ORDER BY\n",
    "words.wordid ASC,\n",
    "postypes.pos ASC,\n",
    "senses.tagcount DESC,\n",
    "senses.sensenum ASC;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The main function.\"\"\"\n",
    "logger.info(\"Reading dataframe from csv...\")\n",
    "orig_dict_df = pd.read_csv(\n",
    "    input_data_dir + \"dict_compact_wordnet.csv\", encoding='utf-8')\n",
    "\n",
    "log_obj(logging.INFO, orig_dict_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLP module.\n",
    "nlp = spacy.load('en_core_web_sm')  # for efficiency.\n",
    "# nlp = spacy.load('en_core_web_trf')  # for accuracy.\n",
    "\n",
    "# Select necessary NLP pipe components.\n",
    "# default selection takes around 6 mins.\n",
    "# custom selection takes around 1.5 mins.\n",
    "nlp.select_pipes(enable=['tagger', 'attribute_ruler', 'lemmatizer'])\n",
    "logger.info(f\"{nlp.pipe_names = }\")\n",
    "logger.info(f\"{nlp.analyze_pipes() = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The following is a cool example of adding a custom NLP pipe to spacy.\n",
    "# # However, it turns out what I want can be accomplished much more simply.\n",
    "\n",
    "# #### Add a custom NLP pipe for lowercasing.\n",
    "# # Add the attribute to store the NLP pi[e] result.\n",
    "# if not spacy.tokens.Token.has_extension('lower_'):\n",
    "#     spacy.tokens.Token.set_extension('lower_', default='')\n",
    "\n",
    "# # Define the actual NLP pipe.\n",
    "# @spacy.language.Language.component('lowercaser')\n",
    "# def lowercaser(doc):\n",
    "#    # Do something to the doc here\n",
    "#    for token in doc:\n",
    "#        token._.lower_ = token.lemma_.lower()\n",
    "#    return doc\n",
    "\n",
    "# # Add the pipe to end of the pipeline.\n",
    "# nlp.add_pipe('lowercaser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply spacy preprocessing.\n",
    "# Note: Spacy lemmatization keeps capitals when not using tagger. Hence, do not disable it.\n",
    "# Remove null term rows. words like \"NaN\" are troublesome, so they are dropped.\n",
    "dict_df = orig_dict_df.dropna(subset=[term_colname])\n",
    "# Use smaller dataset for testing (optional).\n",
    "# dict_df = dict_df.iloc[:50000, :]  # todo: remove.\n",
    "\n",
    "# Apply NLP preprocessing to dictionary terms.\n",
    "preproc_term_docs = [\n",
    "    [token.lemma_.lower()\n",
    "        for token in doc if (token.is_alpha and not token.is_stop)]\n",
    "    for doc in nlp.pipe(dict_df[term_colname])\n",
    "]\n",
    "preproc_term_docs = ['_'.join(doc) for doc in preproc_term_docs]\n",
    "# Apply NLP preprocessing to dictionary definitions.\n",
    "preproc_defn_docs = [\n",
    "    [token.lemma_.lower()\n",
    "        for token in doc if (token.is_alpha and not token.is_stop)]\n",
    "    for doc in nlp.pipe(dict_df[defn_colname])\n",
    "]\n",
    "# Extract the postag information as well. this is useful for WSD.\n",
    "preproc_postags = dict_df[pos_colname]\n",
    "\n",
    "# ## The following variants were added to facilitate PoS tagging for definitions.\n",
    "# ## Tagging results were not very good, so it was removed later.\n",
    "# # Apply NLP preprocessing to dictionary definitions.\n",
    "# preproc_defn_docs = [\n",
    "#     [(token.lemma_.lower(),\n",
    "#         upos_code_dict.get(token.pos_.lower(), upos_code_dict[\"default\"]))\n",
    "#         for token in doc if (token.is_alpha and not token.is_stop)]\n",
    "#     for doc in nlp.pipe(dict_df[defn_colname])\n",
    "# ]\n",
    "# # Extract the postag information as well. this is useful for WSD.\n",
    "# preproc_postags = dict_df[pos_colname].apply(\n",
    "#     lambda x: pos_code_dict.get(x, pos_code_dict[\"default\"]))\n",
    "\n",
    "# Combine terms, postags, and definitions in a table.\n",
    "preproc_dict_table = [[term_doc, posname, defn_doc] for term_doc, posname, defn_doc\n",
    "                      in zip(preproc_term_docs, preproc_postags, preproc_defn_docs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary dataframe from preprocessed dictionary table.\n",
    "preproc_dict_df = pd.DataFrame(copy.deepcopy(preproc_dict_table), columns=[\n",
    "                               term_colname, pos_colname, defn_colname])\n",
    "# Apply dataframe preprocessing.\n",
    "# Remove empty term and definition rows.\n",
    "preproc_dict_df = preproc_dict_df[preproc_dict_df[term_colname].astype(bool)]\n",
    "preproc_dict_df = preproc_dict_df[preproc_dict_df[defn_colname].astype(bool)]\n",
    "preproc_dict_df = preproc_dict_df.reset_index(drop=True)\n",
    "\n",
    "# ## the following two steps are now removed, since we will be using WSD now.\n",
    "# # Keep first n term rows for each term, if it has multiple definitions.\n",
    "# rows_to_keep_per_term = 3\n",
    "# preproc_dict_df = preproc_dict_df.groupby(preproc_dict_df[term_colname]).head(rows_to_keep_per_term)\n",
    "# preproc_dict_df = preproc_dict_df.reset_index()  # not necessary if groupby is done via index column.\n",
    "# # Combine duplicate term rows.\n",
    "# preproc_dict_df = preproc_dict_df.groupby(preproc_dict_df[term_colname]).agg({defn_colname: 'sum'})\n",
    "# preproc_dict_df = preproc_dict_df.reset_index()  # not necessary if groupby is done via index column.\n",
    "\n",
    "# add sense id column to preprocessed dictionary dataframe. useful for WSD.\n",
    "preproc_dict_df.insert(loc=preproc_dict_df.columns.get_loc(defn_colname), column=senseid_colname,\n",
    "                       value=preproc_dict_df.groupby(preproc_dict_df[term_colname]).cumcount())\n",
    "\n",
    "# Print and save preprocessed dictionary dataframe.\n",
    "log_obj(logging.INFO, preproc_dict_df)\n",
    "preproc_dict_df.to_pickle(interm_data_dir + \"preproc_dict_df.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dict_df = pd.read_pickle(interm_data_dir + \"preproc_dict_df.pkl\")\n",
    "\n",
    "preproc_dict_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preproc_dict_df[preproc_dict_df[term_colname] == \"gain\"])\n",
    "display(preproc_dict_df[preproc_dict_df[term_colname] == \"signal\"])\n",
    "\n",
    "display(preproc_dict_df[preproc_dict_df[term_colname] == \"gain\"]\n",
    "        .iloc[2].definition)\n",
    "display(preproc_dict_df[preproc_dict_df[term_colname] == \"signal\"]\n",
    "        .iloc[2].definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synset_name(row):\n",
    "    return f\"{row[term_colname]}.{pos_code_dict[row[pos_colname]]}.{row[senseid_colname]:02d}\"\n",
    "\n",
    "\n",
    "def custom_lesk(row):\n",
    "    if row._name % 1000 == 0:\n",
    "        print(f\"custom_lesk: processing row: {row._name}...\")\n",
    "\n",
    "    context = row[defn_colname]\n",
    "    context_wsd = []\n",
    "\n",
    "    # print(f\"{context = }\")\n",
    "\n",
    "    for token in context:\n",
    "        synsets = preproc_dict_df[preproc_dict_df[term_colname] == token]\n",
    "\n",
    "        if synsets.empty:\n",
    "            continue\n",
    "\n",
    "        # # nltk lesk implementation.\n",
    "        # _, sense = max(\n",
    "        #     ((len(set(context) & set(ss[defn_colname])), ss)\n",
    "        #      for _, ss in synsets.iterrows()),\n",
    "        #     key=lambda x: (x[0], -x[1][senseid_colname])\n",
    "        # )\n",
    "\n",
    "        # pandas lesk implementation. hopefully will be slightly faster.\n",
    "        sense = synsets.loc[synsets.apply(lambda x: len(\n",
    "            set(context) & set(x[defn_colname])), axis=1).idxmax(), :]\n",
    "\n",
    "        context_wsd.append(\n",
    "            f\"{token}.{pos_code_dict[sense[pos_colname]]}.{sense[senseid_colname]:02d}\")\n",
    "\n",
    "    # print(f\"{context_wsd = }\")\n",
    "\n",
    "    return context_wsd\n",
    "\n",
    "\n",
    "# mrd_wsd_slice = slice(74661, 74673)\n",
    "# mrd_wsd_slice = slice(0, 100)\n",
    "mrd_wsd_slice = slice(None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply WSD to preprocessed dictionary dataframe.\n",
    "# Note that this procedure takes time!\n",
    "wsd_dict_df = preproc_dict_df.copy()\n",
    "\n",
    "wsd_dict_df.loc[mrd_wsd_slice, defn_colname] = wsd_dict_df.loc[mrd_wsd_slice, :].apply(\n",
    "    lambda x: custom_lesk(x), axis=1)\n",
    "wsd_dict_df.loc[mrd_wsd_slice, term_colname] = wsd_dict_df.loc[mrd_wsd_slice, :].apply(\n",
    "    lambda x: get_synset_name(x), axis=1)\n",
    "\n",
    "wsd_dict_df = wsd_dict_df.drop(columns=[pos_colname, senseid_colname])\n",
    "\n",
    "# log_obj(logging.INFO, wsd_dict_df)\n",
    "wsd_dict_df.to_pickle(interm_data_dir + \"wsd_dict_df.pkl\")\n",
    "\n",
    "wsd_dict_df.loc[mrd_wsd_slice, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple sanity check.\n",
    "\n",
    "wsd_dict_df = pd.read_pickle(interm_data_dir + \"wsd_dict_df.pkl\")\n",
    "\n",
    "display(wsd_dict_df[wsd_dict_df[term_colname].str.startswith(\"gain.\")])\n",
    "display(wsd_dict_df[wsd_dict_df[term_colname].str.startswith(\"signal.\")])\n",
    "\n",
    "display(wsd_dict_df[wsd_dict_df[term_colname].str.startswith(\"signal.\")]\n",
    "        .iloc[2].definition)\n",
    "display(wsd_dict_df[wsd_dict_df[term_colname].str.startswith(\"field.\")]\n",
    "        .iloc[14].definition)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD2SKB_KRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_dict_df = pd.read_pickle(interm_data_dir + \"wsd_dict_df.pkl\")\n",
    "\n",
    "log_obj(logging.INFO, wsd_dict_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get docs and titles.\n",
    "titles = copy.deepcopy(wsd_dict_df[term_colname].to_list())\n",
    "docs = copy.deepcopy(wsd_dict_df[defn_colname].to_list())\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts.\n",
    "# Note that our docs are already preprocessed and tokenized.\n",
    "# Hence, we replace the analyzer with the identity function.\n",
    "tcv = skl_feat_text.CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "preproc_dict_mtx = tcv.fit_transform(docs)\n",
    "defn_words = tcv.get_feature_names_out()\n",
    "\n",
    "# Reduce memory usage in matrix with casting.\n",
    "preproc_dict_mtx = preproc_dict_mtx.astype(\"uint16\")\n",
    "\n",
    "# Construct index objects.\n",
    "preproc_dict_words = pd.Index(titles)\n",
    "preproc_defn_words = pd.Index(defn_words)\n",
    "\n",
    "# Create dataframe.\n",
    "preproc_dict_mtx_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    preproc_dict_mtx, index=preproc_dict_words, columns=preproc_defn_words)\n",
    "\n",
    "# Print the dictionary matrix dataframe.\n",
    "log_obj(logging.INFO, preproc_dict_mtx_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple sanity check.\n",
    "test_word = 'tweet.'\n",
    "\n",
    "preproc_dict_mtx_df.loc[preproc_dict_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(preproc_dict_mtx_df.columns[x.values > 0]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRD2SKB_KRO algorithm definitions.\n",
    "\n",
    "# @timer_dec\n",
    "# @numba.jit(nopython=True, parallel=True)\n",
    "def compute_dict_mtx_update(dict_mtx, dict_words, defn_words, colsum_min_col_idxs):\n",
    "    # For each key, apply removal procedure.\n",
    "    dict_mtx_update = 0\n",
    "    for colsum_min_col_idx in colsum_min_col_idxs:\n",
    "        # Get column index from key.\n",
    "        colsum_min_key = defn_words[colsum_min_col_idx]\n",
    "\n",
    "        # Check if definition word is also in dict words.\n",
    "        if colsum_min_key in dict_words:\n",
    "            # Perform row operations.\n",
    "            # logger.debug(f\"{colsum_min_key} IS in dict_words.\")\n",
    "            # Get row index corresponding to minimum column sum word.\n",
    "            colsum_min_row_idx = dict_words.get_loc(colsum_min_key)\n",
    "            # colsum_min_row_idx = np.argwhere(dict_words == colsum_min_key)[0][0]\n",
    "\n",
    "            # Using dataframe in loop body was inefficient performance wise.\n",
    "            # Use sparse matrices directly in loop body.\n",
    "            dict_col = dict_mtx[:, colsum_min_col_idx]\n",
    "            dict_row = dict_mtx[colsum_min_row_idx, :]\n",
    "            dict_mtx_single_update = \\\n",
    "                sp.sparse.kron(dict_col, dict_row, format=None)\n",
    "\n",
    "            dict_mtx_update += dict_mtx_single_update\n",
    "        else:\n",
    "            # Do not perform row operations.\n",
    "            # logger.debug(f\"{colsum_min_key} IS NOT in dict_words.\")\n",
    "            pass\n",
    "    return dict_mtx_update\n",
    "\n",
    "\n",
    "@timer_dec\n",
    "def mrd2skb_kro(dict_mtx_df, hyperparams):\n",
    "    # Set dictionary word and definition term lists.\n",
    "    dict_words = dict_mtx_df.index.copy()\n",
    "    defn_words = dict_mtx_df.columns.copy()\n",
    "    # Set sparse matrix format (csc, csr, etc.).\n",
    "    # \"csc\" seems optimal for performance.\n",
    "    # Probably because column drop operation in each loop iteration.\n",
    "    dict_mtx = dict_mtx_df.sparse.to_coo().tocsc().copy()\n",
    "\n",
    "    # Start procedure iteration.\n",
    "    colsums = None\n",
    "    epoch = 0\n",
    "\n",
    "    prev_dict_mtx_shape = dict_mtx.shape\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        # Check if matrix is empty.\n",
    "        if dict_mtx.size == 0:\n",
    "            logger.info(f\"Matrix is empty, exiting.\")\n",
    "            break\n",
    "        # Compute sparse matrix density.\n",
    "        mtx_sparse_density = dict_mtx.getnnz(\n",
    "        ) / functools.reduce(operator.mul, dict_mtx.shape, 1)\n",
    "        # Compute dict_matrix column sums.\n",
    "        colsums = np.asarray(dict_mtx.sum(axis=0)).ravel()\n",
    "        # Find minimum value of column sums.\n",
    "        colsum_min_val = colsums.min()\n",
    "        # Print procedure pass status info.\n",
    "        # if (epoch % 100 == 0):\n",
    "        if ((prev_dict_mtx_shape[1] - dict_mtx.shape[1]) > 200):\n",
    "            logger.debug(f\"{epoch = }\")\n",
    "            logger.debug(f\"{mtx_sparse_density = }\")\n",
    "            logger.debug(f\"{colsum_min_val = }\")\n",
    "            logger.debug(f\"{dict_mtx.shape[1] = }\")\n",
    "            prev_dict_mtx_shape = dict_mtx.shape\n",
    "\n",
    "        # ## Stopping conditions.\n",
    "        # # Stop if minimum column sum is greater than threshold.\n",
    "        # colsum_thrs = \\\n",
    "        #     hyperparams['colsum_thrs_mult'] * np.divide(*dict_mtx.shape)\n",
    "        # if colsum_min_val >= colsum_thrs:\n",
    "        #     logger.info(f\"{colsum_min_val=} > {hyperparams['colsum_thrs']=}\")\n",
    "        #     logger.info(f\"Threshold reached, exiting.\")\n",
    "        #     break\n",
    "        # # Stop if matrix is denser than threshold.\n",
    "        # if mtx_sparse_density >= hyperparams['sparse_thrs']:\n",
    "        #     logger.info(\n",
    "        #         f\"{mtx_sparse_density=} > {hyperparams['sparse_thrs']=}\")\n",
    "        #     logger.info(f\"Threshold reached, exiting.\")\n",
    "        #     break\n",
    "        # Stop if number of columns is low enough.\n",
    "        if dict_mtx.shape[1] <= hyperparams['num_cols_thrs']:\n",
    "            logger.info(\n",
    "                f\"{dict_mtx.shape[1]=} < {hyperparams['num_cols_thrs']=}\")\n",
    "            logger.info(f\"Threshold reached, exiting.\")\n",
    "            break\n",
    "\n",
    "        # Find minimum column sum indices.\n",
    "        colsum_min_col_idxs = np.where(colsums <= (\n",
    "            colsum_min_val + hyperparams['colsum_thrs_buffer']))[0]\n",
    "\n",
    "        # Compute dictionary matrix update.\n",
    "        dict_mtx_update = compute_dict_mtx_update(\n",
    "            dict_mtx, dict_words, defn_words, colsum_min_col_idxs)\n",
    "\n",
    "        # Scale cumulative updates by discount factor.\n",
    "        dict_mtx_update *= hyperparams['discount_factor']\n",
    "\n",
    "        # Zero out very small cumulative update elements by rounding.\n",
    "        dict_mtx_update = np.around(dict_mtx_update, decimals=2)\n",
    "\n",
    "        # Apply cumulative updates.\n",
    "        dict_mtx += dict_mtx_update\n",
    "\n",
    "        # Drop columns with minimum sum.\n",
    "        # Regular drop method does not exist (?), hence use other methods.\n",
    "        cols_to_keep = list(\n",
    "            set(range(dict_mtx.shape[1])) - set(colsum_min_col_idxs))\n",
    "        dict_mtx = dict_mtx[:, cols_to_keep]\n",
    "\n",
    "        # Remove minimum column sum keys from the dictionary definition index.\n",
    "        defn_words = defn_words.delete(colsum_min_col_idxs)\n",
    "\n",
    "    # Reduce memory usage in matrix with casting (optional).\n",
    "    # dict_mtx = dict_mtx.astype(\"uint16\")\n",
    "\n",
    "    # Prepare dataframe.\n",
    "    mrd2skb_mtx_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "        dict_mtx, index=dict_words, columns=defn_words)\n",
    "\n",
    "    return mrd2skb_mtx_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters.\n",
    "hyperparams = {\n",
    "    # 'colsum_thrs_factor': 1,  # Estimate of # of sememes per word (?).\n",
    "    # 'sparse_thrs': 0.3,  # Estimate of enough matrix density.\n",
    "    'discount_factor': 0.2,  # discount_factor<=1, keep things integer???\n",
    "    'num_cols_thrs': 3000,  # new threshold, lets try.\n",
    "    # this parameter may reduce the number of epochs (virtually).\n",
    "    'colsum_thrs_buffer': 0.0,\n",
    "    # threshold to annotate a word with a sememe.\n",
    "    'sememe_annotation_thrs': 0.9,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate skb matrix for single set of hyperparameters.\n",
    "mrd2skb_mtx_df = mrd2skb_kro(preproc_dict_mtx_df, hyperparams)\n",
    "\n",
    "# Print and save results.\n",
    "log_obj(logging.INFO, mrd2skb_mtx_df)\n",
    "mrd2skb_mtx_df.to_pickle(output_data_dir + \"mrd2skb_mtx_df.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD2SKB Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all MRD2SKB methods, mrd2skb_mtx_df should be generated.\n",
    "# then, the following portions of the code will stay the same.\n",
    "\n",
    "mrd2skb_mtx_df = pd.read_pickle(output_data_dir + \"mrd2skb_mtx_df.pkl\")\n",
    "\n",
    "log_obj(logging.INFO, mrd2skb_mtx_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some simple sanity checks.\n",
    "test_word = 'tweet.'\n",
    "\n",
    "test_annotation = mrd2skb_mtx_df.loc[mrd2skb_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(mrd2skb_mtx_df.columns[x.values > hyperparams['sememe_annotation_thrs']]), axis=1)\n",
    "\n",
    "print(test_annotation)\n",
    "\n",
    "test_word = 'screenwriter.'\n",
    "\n",
    "test_annotation = mrd2skb_mtx_df.loc[mrd2skb_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(mrd2skb_mtx_df.columns[x.values > hyperparams['sememe_annotation_thrs']]), axis=1)\n",
    "\n",
    "print(test_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrd2skb output formatting functions.\n",
    "\n",
    "def parse_sense_str(sense_str: str) -> dict:\n",
    "    sense_dict_keys = [term_colname, pos_colname, senseid_colname]\n",
    "    return dict(zip(sense_dict_keys, sense_str.rsplit('.', 2)))\n",
    "\n",
    "\n",
    "@timer_dec\n",
    "def gen_mrd2skb_list_df(mrd2skb_mtx_df, hyperparams):\n",
    "    mrd2skb_bin_df = (\n",
    "        mrd2skb_mtx_df > hyperparams['sememe_annotation_thrs'])\n",
    "    mrd2skb_list_df = mrd2skb_bin_df.dot(\n",
    "        mrd2skb_bin_df.columns + ' ').str.rstrip().str.split()\n",
    "    return mrd2skb_list_df\n",
    "\n",
    "\n",
    "def generate_txt_outputs(mrd2skb_mtx_df, mrd2skb_list_df):\n",
    "    # save \"mrd2skb_sememes.txt\".\n",
    "    with open(output_data_dir + \"mrd2skb_sememes.txt\", 'w') as fp:\n",
    "        sememes = (parse_sense_str(term)[term_colname]\n",
    "                   for term in mrd2skb_mtx_df.columns)\n",
    "        [fp.write(f\"{sememe}\\n\") for sememe in sorted(set(sememes))]\n",
    "\n",
    "    # merge multiple sense definitions of the same terms.\n",
    "    mrg_list_df = mrd2skb_list_df.copy()\n",
    "    mrg_list_df.index = mrg_list_df.index.map(\n",
    "        lambda x: parse_sense_str(x)[term_colname])\n",
    "    mrg_list_df = mrg_list_df.map(\n",
    "        lambda x: [parse_sense_str(term)[term_colname] for term in x])\n",
    "    mrg_list_df = mrg_list_df.groupby(mrg_list_df.index).sum().apply(\n",
    "        lambda x: sorted(set(x), key=x.index))\n",
    "\n",
    "    # save \"mrd2skb_skb.txt\".\n",
    "    with open(output_data_dir + \"mrd2skb_skb.txt\", 'w') as fp:\n",
    "        for key, value in mrg_list_df.to_dict().items():\n",
    "            if len(value) > 0:\n",
    "                term = parse_sense_str(key)[term_colname]\n",
    "                sememe_set = ' '.join(parse_sense_str(sememe)[term_colname]\n",
    "                                      for sememe in value)\n",
    "                fp.write(f\"{term}\\n{sememe_set}\\n\")\n",
    "\n",
    "\n",
    "def generate_npy_outputs(mrd2skb_mtx_df, mrd2skb_list_df):\n",
    "    # save \"mrd2skb_sememes.npy\".\n",
    "    sememes = (parse_sense_str(term)[term_colname]\n",
    "               for term in mrd2skb_mtx_df.columns)\n",
    "    sememes_np = np.asarray(sorted(set(sememes)))\n",
    "    np.save(output_data_dir + \"mrd2skb_sememes.npy\", sememes_np)\n",
    "\n",
    "    # save \"mrd2skb_valid_words.npy\".\n",
    "    valid_words = (parse_sense_str(term)[term_colname]\n",
    "                   for term in mrd2skb_mtx_df.index)\n",
    "    valid_words_np = np.asarray(sorted(set(valid_words)))\n",
    "    np.save(output_data_dir + \"mrd2skb_valid_words.npy\", valid_words_np)\n",
    "\n",
    "    # save \"mrd2skb_dict.npy\".\n",
    "    mrg_list_df = mrd2skb_list_df.copy()\n",
    "\n",
    "    mrg_list_df = mrg_list_df.map(\n",
    "        lambda x: [parse_sense_str(term)[term_colname] for term in x])\n",
    "\n",
    "    mrg_list_df_idx = mrd2skb_list_df.index.str.rsplit(\n",
    "        '.', n=2).map(lambda x: x[0])\n",
    "\n",
    "    mrg_list_df = mrg_list_df.groupby(mrg_list_df_idx).apply(\n",
    "        lambda x: [(pos_code_inv_dict[sense.rsplit('.', 2)[1]], set(defn)) for sense, defn in zip(x.index, x)])\n",
    "\n",
    "    mrd2skb_dict = mrg_list_df.to_dict()\n",
    "    np.save(output_data_dir + \"mrd2skb_dict.npy\", mrd2skb_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate skb list dataframe with binary sememe annotations.\n",
    "mrd2skb_list_df = gen_mrd2skb_list_df(mrd2skb_mtx_df, hyperparams)\n",
    "\n",
    "# Print and save results.\n",
    "log_obj(logging.INFO, mrd2skb_list_df)\n",
    "mrd2skb_list_df.to_pickle(output_data_dir + \"mrd2skb_list_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrd2skb_list_df = pd.read_pickle(output_data_dir + \"mrd2skb_list_df.pkl\")\n",
    "\n",
    "log_obj(logging.INFO, mrd2skb_list_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and save skb outputs. these files are used in ThuNLP DictSKB code.\n",
    "generate_txt_outputs(mrd2skb_mtx_df, mrd2skb_list_df)\n",
    "generate_npy_outputs(mrd2skb_mtx_df, mrd2skb_list_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for the skb file outputs.\n",
    "mrd2skb_sememes = np.load(output_data_dir + \"mrd2skb_sememes.npy\")\n",
    "mrd2skb_valid_words = np.load(output_data_dir + \"mrd2skb_valid_words.npy\")\n",
    "mrd2skb_dict = np.load(\n",
    "    output_data_dir + \"mrd2skb_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "print(f\"{mrd2skb_sememes.size = }\")\n",
    "# print(f\"{mrd2skb_sememes = }\")\n",
    "print(f\"{type(mrd2skb_sememes) = }\")\n",
    "\n",
    "print(f\"{mrd2skb_valid_words.size = }\")\n",
    "# print(f\"{mrd2skb_valid_words = }\")\n",
    "print(f\"{type(mrd2skb_valid_words) = }\")\n",
    "\n",
    "print(f\"{len(mrd2skb_dict) = }\")\n",
    "print(f\"{mrd2skb_dict['tweet'] = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup functions.\n",
    "\n",
    "def get_hparam_hash(hyperparams):\n",
    "    hparams_json = json.dumps(hyperparams).encode(\"utf-8\")\n",
    "    hparams_hash = hashlib.md5(hparams_json).hexdigest()\n",
    "    return hparams_hash\n",
    "\n",
    "\n",
    "def backup_data(hyperparams):\n",
    "    hparam_str = \"_\".join(\n",
    "        [f\"x{idx}={val}\" for idx, (key, val) in enumerate(hyperparams.items())])\n",
    "    cur_backup_dirname = f\"mrd2skb_kro_bkp_{hparam_str}/\"\n",
    "    cur_backup_dir = backup_data_dir + cur_backup_dirname\n",
    "    pathlib.Path(cur_backup_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(cur_backup_dir + 'hparams.json', 'w') as fp:\n",
    "        json.dump(hyperparams, fp, ensure_ascii=False)\n",
    "\n",
    "    shutil.copytree(interm_data_dir, cur_backup_dir +\n",
    "                    \"interm/\", dirs_exist_ok=True)\n",
    "    shutil.copytree(output_data_dir, cur_backup_dir +\n",
    "                    \"output/\", dirs_exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup mrd2skb data for the current session.\n",
    "backup_data(hyperparams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRD2SKB Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This part may differ with other MRD2SKB methods.\n",
    "\n",
    "# Generate skb's with hyperparameter grid search, and store results in backup.\n",
    "\n",
    "# # Copy data.\n",
    "# dict_mtx = preproc_dict_mtx.copy()\n",
    "# dict_words = copy.deepcopy(preproc_dict_words)\n",
    "# defn_words = copy.deepcopy(preproc_defn_words)\n",
    "\n",
    "hyperparams = {\n",
    "    # 'colsum_thrs_factor': 1,  # Estimate of # of sememes per word (?).\n",
    "    # 'sparse_thrs': 0.3,  # Estimate of enough matrix density.\n",
    "    'discount_factor': 0.5,  # discount_factor<=1, keep things integer???\n",
    "    'num_cols_thrs': 3000,  # new threshold, lets try.\n",
    "    # this parameter may reduce the number of epochs (virtually).\n",
    "    'colsum_thrs_buffer': 0.01,\n",
    "    # threshold to annotate a word with a sememe.\n",
    "    'sememe_annotation_thrs': 1.0,\n",
    "    # the following is used as another early stop,\n",
    "    # otherwise computations become intractable.\n",
    "    'sparse_thrs': 0.40,\n",
    "}\n",
    "\n",
    "# 1000 quickly becomes very time intensive to compute,\n",
    "# so we phased it out after discount factor 0.5.\n",
    "# num_cols_thrs_list = [1000, 2000, 3000, 4000, 5000]\n",
    "# num_cols_thrs_list = [2000, 3000, 4000, 5000]\n",
    "num_cols_thrs_list = [4000, 5000]\n",
    "discount_factor_list = [0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "# sememe_annotation_thrs = 0 is not very logical, removed it.\n",
    "sememe_annotation_thrs_list = [0.20, 0.40, 0.60, 0.80, 1.00]\n",
    "\n",
    "for num_cols_thrs in num_cols_thrs_list:\n",
    "    hyperparams['num_cols_thrs'] = num_cols_thrs\n",
    "    for discount_factor in discount_factor_list:\n",
    "        hyperparams['discount_factor'] = discount_factor\n",
    "        logger.info(f\"{num_cols_thrs=}, {discount_factor=}.\")\n",
    "\n",
    "        mrd2skb_mtx_df = mrd2skb_kro(preproc_dict_mtx_df, hyperparams)\n",
    "        for sememe_annotation_thrs in sememe_annotation_thrs_list:\n",
    "            hyperparams['sememe_annotation_thrs'] = sememe_annotation_thrs\n",
    "            logger.info(f\"{sememe_annotation_thrs=}.\")\n",
    "            mrd2skb_list_df = gen_mrd2skb_list_df(mrd2skb_mtx_df, hyperparams)\n",
    "            generate_txt_outputs(mrd2skb_mtx_df, mrd2skb_list_df)\n",
    "            generate_npy_outputs(mrd2skb_mtx_df, mrd2skb_list_df)\n",
    "            backup_data(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload a backed up mrd2skb matrix.\n",
    "\n",
    "mrd2skb_mtx_df = pd.read_pickle(backup_data_dir + \"mrd2skb_kro_bkp_x0=0.2_x1=3000_x2=0.0_x3=0.9/\" + \"output/\" + \"mrd2skb_mtx_df.pkl\")\n",
    "# mrd2skb_mtx_df = pd.read_pickle(backup_data_dir + \"mrd2skb_top_bkp_x0=Nmf_x1=100_x2=0/\" + \"output/\" + \"mrd2skb_mtx_df.pkl\")\n",
    "# mrd2skb_mtx_df = pd.read_pickle(backup_data_dir + \"mrd2skb_top_bkp_x0=LsiModel_x1=200_x2=0/\" + \"output/\" + \"mrd2skb_mtx_df.pkl\")\n",
    "# mrd2skb_mtx_df = pd.read_pickle(backup_data_dir + \"mrd2skb_top_bkp_x0=PLsaModel_x1=100_x2=0/\" + \"output/\" + \"mrd2skb_mtx_df.pkl\")\n",
    "# mrd2skb_mtx_df = pd.read_pickle(backup_data_dir + \"mrd2skb_top_bkp_x0=LdaModel_x1=100_x2=0/\" + \"output/\" + \"mrd2skb_mtx_df.pkl\")\n",
    "\n",
    "# Extract some qualitative examples from the given skb.\n",
    "\n",
    "sememe_ann_thrs = 0.9\n",
    "\n",
    "test_word = 'tweet.'\n",
    "\n",
    "test_annotations = mrd2skb_mtx_df.loc[mrd2skb_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(mrd2skb_mtx_df.columns[x.values > sememe_ann_thrs]), axis=1)\n",
    "\n",
    "[print(f\"{test_word} = {test_annotation}\") for test_annotation in test_annotations]\n",
    "\n",
    "test_word = 'screenwriter.'\n",
    "\n",
    "test_annotations = mrd2skb_mtx_df.loc[mrd2skb_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(mrd2skb_mtx_df.columns[x.values > sememe_ann_thrs]), axis=1)\n",
    "\n",
    "[print(f\"{test_word} = {test_annotation}\") for test_annotation in test_annotations]\n",
    "\n",
    "test_word = 'hospital.'\n",
    "\n",
    "test_annotations = mrd2skb_mtx_df.loc[mrd2skb_mtx_df.index.str.startswith(test_word)].apply(\n",
    "    lambda x: list(mrd2skb_mtx_df.columns[x.values > sememe_ann_thrs]), axis=1)\n",
    "\n",
    "[print(f\"{test_word} = {test_annotation}\") for test_annotation in test_annotations]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations\n",
    "\n",
    "To perform the evaluations, download the DictSKB repo from Github: https://github.com/thunlp/DictSKB\n",
    "\n",
    "Prepare an appropriate conda environment for each evaluation task, as per documented in the repo itself.\n",
    "\n",
    "Then, perform the following file substitutions to obtain the evaluation results for MRD2SKB:\n",
    "\n",
    "- adversarial_attack/core_sememe_dict.npy <- mrd2skb_dict.npy\n",
    "- consistency_check/core_sememe_dict.npy <- mrd2skb_dict.npy\n",
    "- lm_sdlm/sememe_dict.uncased.npy <- mrd2skb_dict.npy\n",
    "\n",
    "- consistency_check/dict_sememes.npy <- mrd2skb_sememes.npy\n",
    "- consistency_check/dict_sememes.npy <- mrd2skb_valid_words.npy\n",
    "\n",
    "- nli/sememe_dict.txt <- mrd2skb_skb.txt\n",
    "- nli/sememes.txt <- mrd2skb_sememes.txt\n",
    "\n",
    "In order not to change the code in DictSKB, keep the names of the files the same, just overwrite the contents.\n",
    "\n",
    "Then, perform the evaluations as described in the DictSKB repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1ce64c2b35e2ec82aa358cee80e6b28a966da108a18834680f418c57a012dd8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
